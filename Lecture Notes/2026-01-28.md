# 2026-01-28

## Review

Last time, we went over instertion sort, which scales O(n^2), which is pretty slow. Today we are going to find out if we can do any better.

## Merge Sort

Merge Sort is a divide and conquer approach, like we did with multiplication, but for sorting.

We take our list, divide it into 2 pieces, then we recurse and sort the pieces, then merge.

Merge Sort Pseudo Code:
```python
MERGESORT(A):
    n = length(A)

    if n <= 1:
        return A

    L = MERGESORT(A[1 : n/2])
    R = MERGESORT(A[n/s+1 : n])

    return MERGE(L,R)
```
When we merge, we basically go back up recursivly and sort along the way.

- Inductive hypothesis: "in every recrusive call on an array of length at most i, mergesort returns a sorted array."
- Base case: a 1-element array is always sorted.

MergeSort requires at most 11n (log(n) + 1) operations to sort.

The scaling power of log is very powerful. So, if an algorithm runs in log(n), that's a good sign.

### Merging

Operations we may have to do:
- Comparison
- Appending
    - Write to temp
    - Allocating
- Iterating
- Initializing pointers
- Clear memory
- Dereference pointers

In total, all the operations we have to do ends up adding up to 11k operations per subproblem.

At each level of our tree, we have to do 11n operations. At the last level, we have to do 2n operations.

So, since we have 11n steps per level and log(n) + 1 levels, mergesort will have 11n (log(n) + 1) operations.

```math
11n(log(n) + 1)
= 11nlog(n) + 11n
```

We choose O(nlog(n)) since over time, nlog(n) is faster than n.

# Omega Notation

omega(...) is the lower bound.

```math
n\log_{2}(n) = \Omega(3n) \\
T(n) = \Omega(g(n))
```
choose:
```math
c = \frac{1}{3} \\
n_0 = 2
```
then:
```math
An \ge 2 \\
0 \le \frac{3n}{3} \le nlog_2(n)
```