# 2026-02-02

## Solving Recurrences and Master Theorem

### Asymptotic Bounds

- Let T(n), g(n) be functions of positive integers
- We say "T(n) is O(g(n))" if T(n) grows no faster than g(n) as n gets large.
- We say "T(n) is omega(g(n))" if T(n) grows at least as fast as g(n) as n gets large.

**Example:** Formally prove
```math
2n^2 + 10 = O(n^2)
```

- Choose n0 = 4 and c = 3.
- Claim: For all n >= 4, we have to 0 <= 2 * n^2 +10 <= 3 * n^2
- To prove the claim, first notice that for n >= 4,
```math
2 \cdot n^2 + 10 \le 3 \cdot n^2 \\
10 \le n^2 \\
\sqrt{10} \le n
```

- This last thing is true for any n >= 4 since sqrt(10) is about 3.16 <= 4.
- We also have 0 <= 2 * n^2 = 10 for all n, since n^2 >= 0 is always positive.

**Example:** Polynomials

- Suppose the p(n) is a polynomial of degree k:
```math
p(n) = a_0 + a_1n + a_2n + ... + a_kn^k
```
where ak > 0.
- Then p(n) = O(n^k)
- Proof
    - Choose n0 >= 1 so that p(n) >= 0 for all n >= n0.
    - Choose c = |a0| + |a1| + ... + |ak|
```math
0 \le p(n) \le |p(n)| \le |a_0| + |a_1|n + ... + |a_k|n^k \\
\le |a_0|n^k + |a_1|n^k + ... + |a_k|n^k \\
= c \cdot n^k
```

---

- To prove T(n) = O(g(n)), you have to ocme up with c and n0 so that hte definition is satisfied.
- To prove T(n) is NOT O(g(n)), one way is **proof by contradiction**.

### Master Theorem

- Suppose that a >= 1, b > 1, and d are constants (independant of n).
- Suppose T(n) = a * T(n/b) + O(n^d). Then:
```math
T(n) = \\
O(n^dlog(n)) \ \ \ \text{ if } a = b^d \\
O(n^d) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{ if } a < b^d\\
O(n^{log_b(a)}) \ \ \ \ \ \ \ \text{ if } a > b^d
```